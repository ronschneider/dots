{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy won 10 matches, Random won 0\n",
      "Greedy won 10 matches, Random won 0\n",
      "start\n",
      "Tensor(\"X:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"hidden/Relu:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"q_values/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "nn_input = [False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1092\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3578\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3579\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a NoneType into a Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bd9e3c4972f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m \u001b[0mtest_qplayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bd9e3c4972f8>\u001b[0m in \u001b[0;36mtest_qplayer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mwinner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a3595b586067>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_moves\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m#print(b.print())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;31m#print(\"{} selects {}\".format(self.current_player().symbol, move))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bd9e3c4972f8>\u001b[0m in \u001b[0;36mselect_move\u001b[0;34m(self, board)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn_input = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bd9e3c4972f8>\u001b[0m in \u001b[0;36mget_probs\u001b[0;34m(self, input_pos)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m    146\u001b[0m         probs, qvalues = TFSessionManager.get_session().run([self.nn.output_logits, self.nn.q_values],\n\u001b[0;32m--> 147\u001b[0;31m                                                 feed_dict={self.nn.input_positions: [input_pos]})\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1095\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%run dots_model.ipynb\n",
    "%run dots_training_data.ipynb\n",
    "%run TFSessionManager.ipynb\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Contains a TensorFlow graph which is suitable for learning the Tic Tac Toe Q function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Constructor for QNetwork. Takes a name and a learning rate for the GradientDescentOptimizer\n",
    "        :param name: Name of the network\n",
    "        :param learning_rate: Learning rate for the GradientDescentOptimizer\n",
    "        \"\"\"\n",
    "        self.learningRate = learning_rate\n",
    "        self.name = name\n",
    "        self.input_positions = None\n",
    "        self.target_input = None\n",
    "        self.q_values = None\n",
    "        self.predictions = None\n",
    "        self.train_step = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        self.correct_prediction = None\n",
    "        self.accuracy = None\n",
    "        self.output_logits = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.net = None\n",
    "        self.build_graph(name)\n",
    "        #self.saver = tf.train.Saver()\n",
    "\n",
    "    def build_graph(self, name: str):\n",
    "        \"\"\"\n",
    "        Builds a new TensorFlow graph with scope `name`\n",
    "        :param name: The scope for the graph. Needs to be unique for the session.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, BOARD_EDGES], name='X')\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None, BOARD_EDGES], name='Y')\n",
    "\n",
    "            self.net = self.x\n",
    "            print(self.net)\n",
    "            self.net = tf.layers.dense(self.net, BOARD_EDGES*BOARD_EDGES, tf.nn.relu,\n",
    "                                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                           name='hidden')\n",
    "            print(self.net)\n",
    "            self.output_logits = tf.layers.dense(self.net, BOARD_EDGES, None,\n",
    "                                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                           name='q_values')\n",
    "            print(self.output_logits)\n",
    "            self.predictions = tf.nn.softmax(self.output_logits, name='predictions')\n",
    "\n",
    "\n",
    "            # Define the loss function, optimizer, and accuracy\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output_logits), name='loss')\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learningRate, name='Adam-op').minimize(self.loss)\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.output_logits, 1), tf.argmax(self.y, 1), name='correct_pred')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "            #self.saver = tf.train.Saver()\n",
    "\n",
    "class NNQPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a Tic Tac Toe player based on a Reinforcement Neural Network learning the Tic Tac Toe Q function\n",
    "    \"\"\"\n",
    "\n",
    "    def board_state_to_nn_input(self, b) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts a Tic Tac Tow board state to an input feature vector for the Neural Network. The input feature vector\n",
    "        is a bit array of size 27. The first 9 bits are set to 1 on positions containing the player's pieces, the second\n",
    "        9 bits are set to 1 on positions with our opponents pieces, and the final 9 bits are set on empty positions on\n",
    "        the board.\n",
    "        :param state: The board state that is to be converted to a feature vector.\n",
    "        :return: The feature vector representing the input Tic Tac Toe board state.\n",
    "        \"\"\"\n",
    "        return np.array(b)\n",
    "\n",
    "    def __init__(self, name: str, reward_discount: float = 0.95, win_value: float = 1.0, draw_value: float = 0.0,\n",
    "                 loss_value: float = -1.0, learning_rate: float = 0.01, training: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor for the Neural Network player.\n",
    "        :param name: The name of the player. Also the name of its TensorFlow scope. Needs to be unique\n",
    "        :param reward_discount: The factor by which we discount the maximum Q value of the following state\n",
    "        :param win_value: The reward for winning a game\n",
    "        :param draw_value: The reward for playing a draw\n",
    "        :param loss_value: The reward for losing a game\n",
    "        :param learning_rate: The learning rate of the Neural Network\n",
    "        :param training: Flag indicating if the Neural Network should adjust its weights based on the game outcome\n",
    "        (True), or just play the game without further adjusting its weights (False).\n",
    "        \"\"\"\n",
    "        self.reward_discount = reward_discount\n",
    "        self.win_value = win_value\n",
    "        self.draw_value = draw_value\n",
    "        self.loss_value = loss_value\n",
    "        self.side = None\n",
    "        self.board_position_log = []\n",
    "        self.action_log = []\n",
    "        self.next_max_log = []\n",
    "        self.values_log = []\n",
    "        self.name = name\n",
    "        self.nn = Model(name, learning_rate)\n",
    "        self.training = training\n",
    "        super().__init__(name, BLACK) # FIX ME\n",
    "\n",
    "    def new_game(self, side: int):\n",
    "        \"\"\"\n",
    "        Prepares for a new games. Store which side we play and clear internal data structures for the last game.\n",
    "        :param side: The side it will play in the new game.\n",
    "        \"\"\"\n",
    "        self.side = side\n",
    "        self.board_position_log = []\n",
    "        self.action_log = []\n",
    "        self.next_max_log = []\n",
    "        self.values_log = []\n",
    "\n",
    "    def calculate_targets(self) -> [np.ndarray]:\n",
    "        \"\"\"\n",
    "        Based on the recorded moves, compute updated estimates of the Q values for the network to learn\n",
    "        \"\"\"\n",
    "        game_length = len(self.action_log)\n",
    "        targets = []\n",
    "\n",
    "        for i in range(game_length):\n",
    "            target = np.copy(self.values_log[i])\n",
    "\n",
    "            target[self.action_log[i]] = self.reward_discount * self.next_max_log[i]\n",
    "            targets.append(target)\n",
    "\n",
    "        #print(target)\n",
    "        return targets\n",
    "\n",
    "    def get_probs(self, input_pos: np.ndarray) -> ([float], [float]):\n",
    "        \"\"\"\n",
    "        Feeds the feature vector `input_pos` which encodes a board state into the Neural Network and computes the\n",
    "        Q values and corresponding probabilities for all moves (including illegal ones).\n",
    "        :param input_pos: The feature vector to be fed into the Neural Network.\n",
    "        :return: A tuple of probabilities and q values of all actions (including illegal ones).\n",
    "        \"\"\"\n",
    "        probs, qvalues = TFSessionManager.get_session().run([self.nn.output_logits, self.nn.q_values],\n",
    "                                                feed_dict={self.nn.input_positions: [input_pos]})\n",
    "        return probs[0], qvalues[0]\n",
    "\n",
    "    def select_move(self, board: Board):\n",
    "        \"\"\"\n",
    "        Implements the Player interface and makes a move on Board `board`\n",
    "        :param board: The Board to make a move on\n",
    "        :return: A tuple of the GameResult and a flag indicating if the game is over after this move.\n",
    "        \"\"\"\n",
    "\n",
    "        # We record all game positions to feed them into the NN for training with the corresponding updated Q\n",
    "        # values.\n",
    "        nn_input = self.board_state_to_nn_input(board.b)\n",
    "        self.board_position_log.append(np.copy(nn_input))\n",
    "\n",
    "        print(\"nn_input = {}\".format(nn_input))\n",
    "        probs, qvalues = self.get_probs(nn_input)\n",
    "        qvalues = np.copy(qvalues)\n",
    "\n",
    "        # We filter out all illegal moves by setting the probability to -1. We don't change the q values\n",
    "        # as we don't want the NN to waste any effort of learning different Q values for moves that are illegal\n",
    "        # anyway.\n",
    "        #print(\"before \" + str(probs))\n",
    "        #print(\"valid \" + str(board.valid_moves))\n",
    "        for index, p in enumerate(qvalues):\n",
    "            if not index in board.valid_moves:\n",
    "            #if not board.is_legal(index):\n",
    "                probs[index] = -1\n",
    "        #print(\"after \" + str(probs))\n",
    "\n",
    "        # Our next move is the one with the highest probability after removing all illegal ones.\n",
    "        move = np.argmax(probs)  # int\n",
    "        #print(\"move=\" + str(move))\n",
    "        \n",
    "        # Unless this is the very first move, the Q values of the selected move is also the max Q value of\n",
    "        # the move that got the game from the previous state to this one.\n",
    "        if len(self.action_log) > 0:\n",
    "            self.next_max_log.append(qvalues[move])\n",
    "\n",
    "        # We record the action we selected as well as the Q values of the current state for later use when\n",
    "        # adjusting NN weights.\n",
    "        self.action_log.append(move)\n",
    "        self.values_log.append(qvalues)\n",
    "\n",
    "        # We execute the move and return the result\n",
    "        #_, res, finished = board.move(move, self.side)\n",
    "        #return res, finished\n",
    "        return move\n",
    "\n",
    "    def final_result(self, winner):\n",
    "        \"\"\"\n",
    "        This method is called once the game is over. If `self.training` is True, we execute a training run for\n",
    "        the Neural Network.\n",
    "        :param result: The result of the game that just finished.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the final reward based on the game outcome\n",
    "        if (winner == self.ind):\n",
    "            reward = self.win_value  # type: float\n",
    "        else:\n",
    "            reward = self.loss_value  # type: float\n",
    "\n",
    "        #print(\"reward = \" + str(reward))\n",
    "        # The final reward is also the Q value we want to learn for the action that led to it.\n",
    "        self.next_max_log.append(reward)\n",
    "\n",
    "        # If we are in training mode we run the optimizer.\n",
    "        if self.training:\n",
    "            # We calculate our new estimate of what the true Q values are and feed that into the network as\n",
    "            # learning target\n",
    "            targets = self.calculate_targets()\n",
    "\n",
    "            # We convert the input states we have recorded to feature vectors to feed into the training.\n",
    "            nn_input = [self.board_state_to_nn_input(x) for x in self.board_position_log]\n",
    "\n",
    "            print(self.nn.input)\n",
    "            # We run the training step with the recorded inputs and new Q value targets.\n",
    "            TFSessionManager.get_session().run([self.nn.train_step],\n",
    "                                   feed_dict={self.nn.input_positions: nn_input, self.nn.target_input: targets})\n",
    "            \n",
    "\n",
    "def test_qplayer():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    qplayer = NNQPlayer(\"qtest1\", 0.01)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        #saver.restore(sess, \"/tmp/dots_model-9\")\n",
    "\n",
    "        greedy = GreedyPlayer(\"Greedy\", BLACK)\n",
    "        \n",
    "        greedy.ind = 1\n",
    "        qplayer.ind = 2\n",
    "        \n",
    "        for _ in range(1):\n",
    "            g = Game(greedy, qplayer)\n",
    "            winner = g.play_game()\n",
    "            print(g.b.print_board())\n",
    "\n",
    "    if false:\n",
    "        toPlay = 2\n",
    "        while (len(b.valid_moves) > 0):\n",
    "            move = greedy.select_move(b)\n",
    "\n",
    "            floats = [1.0 if x else 0.0 for x in b.b]\n",
    "            labels = [0.0 if x == move else 0.0 for x in range(BOARD_EDGES)]\n",
    "\n",
    "            if toPlay == 1:\n",
    "                move = random.select_move(b)\n",
    "                if not b.move(random.ind, move):\n",
    "                    toPlay = 2\n",
    "            else:\n",
    "                # Run optimization op (backprop)\n",
    "                feed_dict_batch = {x: [floats]}\n",
    "                #predictions = sess.run([predictions, cls_prediction], feed_dict=feed_dict_batch)\n",
    "\n",
    "                #print('before\\n', b.print())\n",
    "                qvalues, prediction = sess.run([predictions, cls_prediction], feed_dict={x: [floats]})\n",
    "\n",
    "                q = qvalues[0]\n",
    "                for index, p in enumerate(q):\n",
    "                    #print(index)\n",
    "                    if not index in b.valid_moves:\n",
    "                        #print(\"dropping index \", str(index))\n",
    "                        q[index] = -1\n",
    "\n",
    "                move = np.argmax(q)  # int\n",
    "                if not move in b.valid_moves:\n",
    "                    print(\"best move is not legal!\")\n",
    "                    #print(b.valid_moves)\n",
    "                    #print(q)\n",
    "\n",
    "                print('before\\n', b.print())\n",
    "                if not b.move(greedy.ind, move):\n",
    "                    toPlay = 1\n",
    "                print('after\\n', b.print())\n",
    "        \n",
    "print(\"start\")\n",
    "test_qplayer()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    qplayer = NNQPlayer(\"Q\")\n",
    "    nn = qplayer.nn\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        #saver.restore(sess, \"/tmp/dots_model-9\")\n",
    "\n",
    "\n",
    "        random = GreedyPlayer(\"Random\", 1)\n",
    "        greedy = GreedyPlayer(\"Greedy\", 2)\n",
    "\n",
    "        print(\"Hello\")\n",
    "        for _ in range(1):\n",
    "            g = Game(random, greedy)\n",
    "            b = g.board\n",
    "\n",
    "            toPlay = 2\n",
    "            while (len(b.valid_moves) > 0):\n",
    "                move = greedy.select_move(b)\n",
    "\n",
    "                floats = [1.0 if x else 0.0 for x in b.b]\n",
    "                labels = [0.0 if x == move else 0.0 for x in range(BOARD_EDGES)]\n",
    "\n",
    "                if toPlay == 1:\n",
    "                    move = random.select_move(b)\n",
    "                    if not b.move(random.ind, move):\n",
    "                        toPlay = 2\n",
    "                else:\n",
    "                    # Run optimization op (backprop)\n",
    "                    feed_dict_batch = {nn.x: [floats]}\n",
    "                    #predictions = sess.run([predictions, cls_prediction], feed_dict=feed_dict_batch)\n",
    "\n",
    "                    #print('before\\n', b.print())\n",
    "                    qvalues, prediction = sess.run([nn.predictions, nn.cls_prediction], feed_dict={nn.x: [floats]})\n",
    "\n",
    "                    q = qvalues[0]\n",
    "                    for index, p in enumerate(q):\n",
    "                        #print(index)\n",
    "                        if not index in b.valid_moves:\n",
    "                            #print(\"dropping index \", str(index))\n",
    "                            q[index] = -1\n",
    "\n",
    "                    move = np.argmax(q)  # int\n",
    "                    if not move in b.valid_moves:\n",
    "                        print(\"best move is not legal!\")\n",
    "                        #print(b.valid_moves)\n",
    "                        #print(q)\n",
    "\n",
    "                    print('before\\n', b.print())\n",
    "                    if not b.move(greedy.ind, move):\n",
    "                        toPlay = 1\n",
    "                    print('after\\n', b.print())\n",
    "    print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building Q\n",
      "Tensor(\"X_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"hidden_1/Relu:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"q_values_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "starting session\n",
      "Training epoch: 1\n",
      "iter   0:\t Loss=3.30,\tTraining Accuracy=17.1%\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=1.35,\tTraining Accuracy=62.4%\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=1.27,\tTraining Accuracy=63.9%\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=1.13,\tTraining Accuracy=69.9%\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=1.14,\tTraining Accuracy=67.6%\n",
      "Training epoch: 6\n",
      "iter   0:\t Loss=1.06,\tTraining Accuracy=68.6%\n",
      "Training epoch: 7\n",
      "iter   0:\t Loss=1.06,\tTraining Accuracy=68.1%\n",
      "Training epoch: 8\n",
      "iter   0:\t Loss=1.03,\tTraining Accuracy=69.3%\n",
      "Training epoch: 9\n",
      "iter   0:\t Loss=1.19,\tTraining Accuracy=63.9%\n",
      "Training epoch: 10\n",
      "iter   0:\t Loss=1.07,\tTraining Accuracy=68.6%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_next_batch(x_set, y_set, start, end):\n",
    "    return x_set[start : end], y_set[start : end]\n",
    "\n",
    "def build_and_save_model():\n",
    "    print(\"building Q\")\n",
    "    qplayer = NNQPlayer(\"Q\")\n",
    "    nn = qplayer.nn\n",
    "\n",
    "    epochs = 10            # Total number of training epochs\n",
    "    batch_size = 1000        # Training batch size\n",
    "    display_freq = 10000      # Frequency of displaying the training results\n",
    "\n",
    "    # Create the op for initializing all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    accs = []\n",
    "    iters = []\n",
    "\n",
    "    starting_ind = 0\n",
    "\n",
    "    print(\"starting session\")\n",
    "    # Launch the graph (session)\n",
    "    #with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('log', TFSessionManager.get_session().graph)\n",
    "    global_step = 0\n",
    "    # Number of training iterations in each epoch\n",
    "    num_tr_iter = 100 #int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print('Training epoch: {}'.format(epoch + 1))\n",
    "\n",
    "        #x_train, y_train = randomize(x_train, y_train)\n",
    "        for iteration in range(num_tr_iter):\n",
    "            global_step += 1\n",
    "            start = iteration * batch_size + starting_ind\n",
    "            end = (iteration + 1) * batch_size + starting_ind\n",
    "            #start = iteration * batch_size\n",
    "            #end = (iteration + 1) * batch_size\n",
    "            #x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "            x_batch, y_batch = generateData(batch_size)\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {nn.x: x_batch, nn.y: y_batch}\n",
    "            sess.run(nn.optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "            if iteration % display_freq == 0:\n",
    "                # Calculate and display the batch loss and accuracy\n",
    "                loss_batch, acc_batch = sess.run([nn.loss, nn.accuracy],\n",
    "                                                 feed_dict=feed_dict_batch)\n",
    "\n",
    "                accs.append(acc_batch)\n",
    "                iters.append(global_step)\n",
    "\n",
    "                print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                      format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    p = plt.plot(iters, accs, 'b-')\n",
    "    plt.show()\n",
    "\n",
    "    #print(\"saving model\")\n",
    "    #save_path = saver.save(sess, \"/tmp/dots_model\", global_step=epoch)\n",
    "    #print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "build_and_save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
