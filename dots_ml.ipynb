{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "QTest won 0 matches, Random won 0\n",
      "QTest won 3 matches, Random won 7\n",
      "QTest won 10 matches, Random won 10\n",
      "QTest won 14 matches, Random won 16\n",
      "QTest won 18 matches, Random won 22\n",
      "QTest won 21 matches, Random won 29\n",
      "QTest won 25 matches, Random won 35\n",
      "QTest won 31 matches, Random won 39\n",
      "QTest won 35 matches, Random won 45\n",
      "QTest won 39 matches, Random won 51\n",
      "QTest won 44 matches, Random won 56\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%run dots_model.ipynb\n",
    "%run dots_training_data.ipynb\n",
    "%run TFSessionManager.ipynb\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Contains a TensorFlow graph which is suitable for learning the Tic Tac Toe Q function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Constructor for QNetwork. Takes a name and a learning rate for the GradientDescentOptimizer\n",
    "        :param name: Name of the network\n",
    "        :param learning_rate: Learning rate for the GradientDescentOptimizer\n",
    "        \"\"\"\n",
    "        self.learningRate = learning_rate\n",
    "        self.name = name\n",
    "        self.input_positions = None\n",
    "        self.target_input = None\n",
    "        self.q_values = None\n",
    "        self.probabilities = None\n",
    "        self.train_step = None\n",
    "        self.loss = None\n",
    "        self.build_graph(name)\n",
    "        #self.saver = tf.train.Saver()\n",
    "\n",
    "    def add_dense_layer(self, input_tensor: tf.Tensor, output_size: int, activation_fn=None,\n",
    "                        name: str = None) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Adds a dense Neural Net layer to network input_tensor\n",
    "        :param input_tensor: The layer to which we should add the new layer\n",
    "        :param output_size: The output size of the new layer\n",
    "        :param activation_fn: The activation function for the new layer, or None if no activation function\n",
    "        should be used\n",
    "        :param name: The optional name of the layer. Useful for saving a loading a TensorFlow graph\n",
    "        :return: A new dense layer attached to the `input_tensor`\n",
    "        \"\"\"\n",
    "        return tf.layers.dense(input_tensor, output_size, activation=activation_fn,\n",
    "                               kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                               name=name)\n",
    "    \n",
    "    def build_graph(self, name: str):\n",
    "        \"\"\"\n",
    "        Builds a new TensorFlow graph with scope `name`\n",
    "        :param name: The scope for the graph. Needs to be unique for the session.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "            self.input_positions = tf.placeholder(tf.float32, shape=[None, BOARD_EDGES], name='X')\n",
    "            self.target_input = tf.placeholder(tf.float32, shape=[None, BOARD_EDGES], name='Y')\n",
    "\n",
    "            net = self.input_positions\n",
    "            net = tf.layers.dense(net, BOARD_EDGES*BOARD_EDGES*10, tf.nn.relu,\n",
    "                                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                           name='hidden')\n",
    "            self.q_values = tf.layers.dense(net, BOARD_EDGES, None,\n",
    "                                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                           name='q_values')\n",
    "\n",
    "            self.probabilities = tf.nn.softmax(self.q_values, name='probabilities')\n",
    "            mse = tf.losses.mean_squared_error(predictions=self.q_values, labels=self.target_input)\n",
    "            self.train_step = tf.train.GradientDescentOptimizer(learning_rate=self.learningRate).minimize(\n",
    "                mse, name='train')\n",
    "\n",
    "class NNQPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a Tic Tac Toe player based on a Reinforcement Neural Network learning the Tic Tac Toe Q function\n",
    "    \"\"\"\n",
    "\n",
    "    def board_state_to_nn_input(self, b) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts a Tic Tac Tow board state to an input feature vector for the Neural Network. The input feature vector\n",
    "        is a bit array of size 27. The first 9 bits are set to 1 on positions containing the player's pieces, the second\n",
    "        9 bits are set to 1 on positions with our opponents pieces, and the final 9 bits are set on empty positions on\n",
    "        the board.\n",
    "        :param state: The board state that is to be converted to a feature vector.\n",
    "        :return: The feature vector representing the input Tic Tac Toe board state.\n",
    "        \"\"\"\n",
    "        return np.array(b)\n",
    "\n",
    "    def __init__(self, name: str, reward_discount: float = 0.95, win_value: float = 1.0, draw_value: float = 0.0,\n",
    "                 loss_value: float = -1.0, learning_rate: float = 0.01, training: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor for the Neural Network player.\n",
    "        :param name: The name of the player. Also the name of its TensorFlow scope. Needs to be unique\n",
    "        :param reward_discount: The factor by which we discount the maximum Q value of the following state\n",
    "        :param win_value: The reward for winning a game\n",
    "        :param draw_value: The reward for playing a draw\n",
    "        :param loss_value: The reward for losing a game\n",
    "        :param learning_rate: The learning rate of the Neural Network\n",
    "        :param training: Flag indicating if the Neural Network should adjust its weights based on the game outcome\n",
    "        (True), or just play the game without further adjusting its weights (False).\n",
    "        \"\"\"\n",
    "        super().__init__(name, BLACK) # FIX ME\n",
    "        \n",
    "        self.reward_discount = reward_discount\n",
    "        self.win_value = win_value\n",
    "        self.draw_value = draw_value\n",
    "        self.loss_value = loss_value\n",
    "        self.side = None\n",
    "        self.board_position_log = []\n",
    "        self.action_log = []\n",
    "        self.next_max_log = []\n",
    "        self.values_log = []\n",
    "        self.name = name\n",
    "        self.nn = Model(name, learning_rate)\n",
    "        self.training = training\n",
    "\n",
    "    def new_game(self):\n",
    "        \"\"\"\n",
    "        Prepares for a new games. Store which side we play and clear internal data structures for the last game.\n",
    "        :param side: The side it will play in the new game.\n",
    "        \"\"\"\n",
    "        self.board_position_log = []\n",
    "        self.action_log = []\n",
    "        self.next_max_log = []\n",
    "        self.values_log = []\n",
    "\n",
    "    def calculate_targets(self) -> [np.ndarray]:\n",
    "        \"\"\"\n",
    "        Based on the recorded moves, compute updated estimates of the Q values for the network to learn\n",
    "        \"\"\"\n",
    "        game_length = len(self.action_log)\n",
    "        targets = []\n",
    "\n",
    "        for i in range(game_length):\n",
    "            target = np.copy(self.values_log[i])\n",
    "\n",
    "            target[self.action_log[i]] = self.reward_discount * self.next_max_log[i]\n",
    "            targets.append(target)\n",
    "\n",
    "        #print(target)\n",
    "        return targets\n",
    "\n",
    "    def get_probs(self, input_pos: np.ndarray) -> ([float], [float]):\n",
    "        \"\"\"\n",
    "        Feeds the feature vector `input_pos` which encodes a board state into the Neural Network and computes the\n",
    "        Q values and corresponding probabilities for all moves (including illegal ones).\n",
    "        :param input_pos: The feature vector to be fed into the Neural Network.\n",
    "        :return: A tuple of probabilities and q values of all actions (including illegal ones).\n",
    "        \"\"\"\n",
    "        probs, qvalues = TFSessionManager.get_session().run([self.nn.probabilities, self.nn.q_values],\n",
    "                                                feed_dict={self.nn.input_positions: [input_pos]})\n",
    "        return probs[0], qvalues[0]\n",
    "\n",
    "    def select_move(self, board: Board):\n",
    "        \"\"\"\n",
    "        Implements the Player interface and makes a move on Board `board`\n",
    "        :param board: The Board to make a move on\n",
    "        :return: A tuple of the GameResult and a flag indicating if the game is over after this move.\n",
    "        \"\"\"\n",
    "\n",
    "        # We record all game positions to feed them into the NN for training with the corresponding updated Q\n",
    "        # values.\n",
    "        nn_input = self.board_state_to_nn_input(board.b)\n",
    "        self.board_position_log.append(np.copy(nn_input))\n",
    "\n",
    "        #print(\"nn_input = {}\".format(nn_input))\n",
    "        probs, qvalues = self.get_probs(nn_input)\n",
    "        qvalues = np.copy(qvalues)\n",
    "\n",
    "        # We filter out all illegal moves by setting the probability to -1. We don't change the q values\n",
    "        # as we don't want the NN to waste any effort of learning different Q values for moves that are illegal\n",
    "        # anyway.\n",
    "        #print(\"before \" + str(probs))\n",
    "        #print(\"valid \" + str(board.valid_moves))\n",
    "        for index, p in enumerate(qvalues):\n",
    "            if not index in board.valid_moves:\n",
    "            #if not board.is_legal(index):\n",
    "                probs[index] = -1\n",
    "        #print(\"after \" + str(probs))\n",
    "\n",
    "        # Our next move is the one with the highest probability after removing all illegal ones.\n",
    "        move = np.argmax(probs)  # int\n",
    "        #print(\"move=\" + str(move))\n",
    "        \n",
    "        # Unless this is the very first move, the Q values of the selected move is also the max Q value of\n",
    "        # the move that got the game from the previous state to this one.\n",
    "        if len(self.action_log) > 0:\n",
    "            self.next_max_log.append(qvalues[move])\n",
    "\n",
    "        # We record the action we selected as well as the Q values of the current state for later use when\n",
    "        # adjusting NN weights.\n",
    "        self.action_log.append(move)\n",
    "        self.values_log.append(qvalues)\n",
    "\n",
    "        # We execute the move and return the result\n",
    "        #_, res, finished = board.move(move, self.side)\n",
    "        #return res, finished\n",
    "        return move\n",
    "\n",
    "    def finalResult(self, winner):\n",
    "        \"\"\"\n",
    "        This method is called once the game is over. If `self.training` is True, we execute a training run for\n",
    "        the Neural Network.\n",
    "        :param result: The result of the game that just finished.\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"final result \" + str(winner))\n",
    "        # Compute the final reward based on the game outcome\n",
    "        if (winner):\n",
    "            reward = self.win_value  # type: float\n",
    "        else:\n",
    "            reward = self.loss_value  # type: float\n",
    "\n",
    "        #print(\"reward = \" + str(reward))\n",
    "        # The final reward is also the Q value we want to learn for the action that led to it.\n",
    "        self.next_max_log.append(reward)\n",
    "\n",
    "        # If we are in training mode we run the optimizer.\n",
    "        if self.training:\n",
    "            # We calculate our new estimate of what the true Q values are and feed that into the network as\n",
    "            # learning target\n",
    "            targets = self.calculate_targets()\n",
    "\n",
    "            # We convert the input states we have recorded to feature vectors to feed into the training.\n",
    "            nn_input = [self.board_state_to_nn_input(x) for x in self.board_position_log]\n",
    "\n",
    "            # We run the training step with the recorded inputs and new Q value targets.\n",
    "            TFSessionManager.get_session().run([self.nn.train_step],\n",
    "                                   feed_dict={self.nn.input_positions: nn_input, self.nn.target_input: targets})\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
